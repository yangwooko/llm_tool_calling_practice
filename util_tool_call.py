import json
import requests
from typing import List, Dict, Any, Callable
import inspect
import os
from dotenv import load_dotenv
from prompts import generate_prompt

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class SimpleToolCaller:
    def __init__(self, TOOLS: List[Dict] = None, TOOL_FUNCTIONS: Dict = None):
        self.TOOLS = TOOLS
        self.TOOL_FUNCTIONS = TOOL_FUNCTIONS
        if os.getenv("USE_OPENAI") == "True":
            self.api_key = os.getenv("OPENAI_API_KEY")
            self.base_url = "https://api.openai.com/v1"
            # self.model = "gpt-3.5-turbo"
            self.model = "gpt-4o-mini-2024-07-18"
        else:
            self.api_key = "EMPTY"
            self.base_url = "https://5c86-109-61-127-28.ngrok-free.app/v1"
            self.model = "Qwen/Qwen3-32B-AWQ"

    def call_llm(self, messages: List[Dict], tools: List[Dict] = None) -> Dict:
        """LLM APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = {"model": self.model, "messages": messages, "temperature": 0.7}

        if tools:
            data["tools"] = tools

        response = requests.post(
            f"{self.base_url}/chat/completions", headers=headers, json=data
        )

        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} - {response.text}")

    def execute_tool(self, tool_call: Dict) -> str:
        """ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸ”§ EXECUTE TOOL-->", tool_call)

        function_name = tool_call["function"]["name"]
        arguments = json.loads(tool_call["function"]["arguments"])

        if function_name in self.TOOL_FUNCTIONS:
            func = self.TOOL_FUNCTIONS[function_name]
            return func(**arguments)
        else:
            return f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {function_name}"

    def chat(self, messages: List[Dict], with_tools: bool = True) -> str:
        """ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”í•©ë‹ˆë‹¤."""
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        if messages[0]["role"] != "system":
            system_messages = generate_prompt("system")
            messages = system_messages + messages

        # ì²« ë²ˆì§¸ LLM í˜¸ì¶œ
        if with_tools:
            response = self.call_llm(messages, self.TOOLS)
        else:
            response = self.call_llm(messages)
        assistant_message = response["choices"][0]["message"]
        messages.append(assistant_message)

        # ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ”ì§€ í™•ì¸ (with_toolsê°€ Trueì¸ ê²½ìš°ì—ë§Œ)
        if with_tools and assistant_message.get("tool_calls"):
            for tool_call in assistant_message["tool_calls"]:
                # ë„êµ¬ ì‹¤í–‰
                tool_result = self.execute_tool(tool_call)

                # ë„êµ¬ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tool_call["id"],
                        "content": tool_result,
                    }
                )

            # ë„êµ¬ ê²°ê³¼ë¥¼ ë°›ì€ í›„ ë‘ ë²ˆì§¸ LLM í˜¸ì¶œ
            final_response = self.call_llm(messages)
            return final_response["choices"][0]["message"]["content"]
        else:
            return assistant_message["content"]
